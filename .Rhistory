V1 <- DictGrams1$unique_entries #number of unique unigrams
V2 <- DictGrams2$unique_entries #number of unique bigrams
distribution <- matrix(unlist(DictGrams2$histogram(4), use.names=FALSE),
ncol= 5, byrow = TRUE)
n2 <- apply(distribution, 2, median)
Y <- n2[1]/(n2[1] + 2*n2[2])
D21  <- 1 - (2*Y*n2[2]/n2[1])
D22  <- 2 - (3*Y*n2[3]/n2[2])
D23  <- 3 - (4*Y*n2[4]/n2[3])
distribution <- matrix(unlist(DictGrams3$histogram(4), use.names=FALSE),
ncol= 5, byrow = TRUE)
n3 <- apply(distribution, 2, median)
Y <- n3[1]/(n3[1] + 2*n3[2])
D31  <- 1 - (2*Y*n3[2]/n3[1])
D32  <- 2 - (3*Y*n3[3]/n3[2])
D33  <- 3 - (4*Y*n3[4]/n3[3])
KN_tri_all <- function(phrase){
#check to see if the singleton words are in the training set or not
#and if not convert them to UNK
if (wc(phrase) == 1){
stop
}
wv <- unlist(strsplit(phrase, " "))
#wv <- c("to", "the", "BOS")
for(i in seq_along(wv)){
if (DictGrams1$query(wv[i]) == 0) wv[i] <- "UNK"
}
#wv
#compute first back-off model (PKN1) (see equation 23 in Chen and Goodman 1998)
if (wv[3] == "UNK") {
PKN1 <- 1/(V1 - 1) #drop EOS from list of unique unigrams
} else {
PKN1 <- bipref$query(wv[3])/V2
}
#compute PKN2
#compute gamma1
#first compute N's
N21  <- DictN21$query(wv[2])
N22  <- DictN22$query(wv[2])
N23  <- DictN23$query(wv[2])
#compute number of trigrams that end with wv[2] and wv[3]
endgram2 <- paste(wv[2], wv[3], sep = " ")
numwv_23 <- tripref$query(endgram2)
if (length(numwv_23 == 0) | numwv_23 == 0) {
numwv_23 <- 0
#PKN2 <- PKN1
}
#then pick which D is used for the discount
if (numwv_23 == 0) {
D <- 0
}  else if (numwv_23 == 1) {
D <- D21
} else if (numwv_23 == 2) {
D <- D22
} else if (numwv_23 >= 3) {
D <- D23
}
#next compute number of trigrams with wv[2] in the middle
numwv_2_ <- trimidd$query(wv[2])
gamma1 <- (D21*N21 + D22*N22 + D23*N23)/numwv_2_
if (gamma1 == 0){
PKN2 <- PKN1
} else {
PKN2 <- max(numwv_23 - D, 0)/numwv_2_ + gamma1*PKN1
}
#compute PKN3
#compute gamma2
#first compute N's
beggram <- paste(wv[1], wv[2], sep = " ")
N31  <- DictN31$query(beggram)
N32  <- DictN32$query(beggram)
N33  <- DictN33$query(beggram)
#compute first part of numerator of PKN3
newphrase <- paste(wv[1], wv[2], wv[3], sep =" ")
numwv123 <- DictGrams3$query(newphrase)
if (length(numwv123) == 0 | numwv123 == 0){
numwv123 <- 0
#  PKN3 <- PKN2
}
#then pick which D is used for the discount
if (numwv123 == 0) {
D <- 0
}  else if (numwv123 == 1) {
D <- D31
} else if (numwv123 == 2) {
D <- D32
} else if (numwv123 >= 3) {
D <- D33
}
#next compute number (not unique) of trigrams ending with wv[3]
numwv__3 <- tricomp$query(wv[3])
gamma2 <- (D31*N31 + D32*N32 + D33*N33)/numwv__3
if (gamma2 == 0){
PKN3 <- PKN2
} else {
PKN3 <- max(numwv123 - D, 0)/numwv__3 + gamma2*PKN2
}
return(PKN3)
}
#read in vector of all unigrams used in model
#calculate the proabability each one completes a three-word phrase
#create list of bigram vectors by sentence to calculate probability
#of training set to test if KN function is working correctly
#and find the wordwith the highest probability
options(max.print = 1000)
counts1 <- readRDS("medium_rev_counts1.rds")
counts1 <- select(counts1, word1)
counts1 <- filter(counts1, word1 != "BOS")
lengcount <- dim(counts1)[1]
maxprob <- function(wrd1, wrd2){
x <- paste(wrd1,wrd2, sep = " ")
y <- rep(x, lengcount)
z <- cbind(y, counts1)
z <- mutate(z, phrase = paste(y, word1))
z <- select(z, phrase)
z$prob <- sapply(z$phrase, KN_tri_all)
result <- z$phrase[which.max(z$prob)]
return(result)
}
#testing
x <- paste("walking", "from", sep = " ")
y <- rep(x, lengcount)
z <- cbind(y, counts1)
z <- mutate(z, phrase = paste(y, word1))
z <- select(z, phrase)
z$prob <- sapply(z$phrase, KN_tri_all)
result <- z$phrase[which.max(z$prob)]
maxprob("walking from")
maxprob("walking", "from")
counts1 <- readRDS("medium_rev_counts1.rds")
counts1 <- select(counts1, word1)
counts1 <- filter(counts1, word1 != "BOS")
lengcount <- dim(counts1)[1]
maxprob <- function(wrd1, wrd2){
x <- paste(wrd1,wrd2, sep = " ")
y <- rep(x, lengcount)
z <- cbind(y, counts1)
z <- mutate(z, phrase = paste(y, word1))
z <- select(z, phrase)
z$prob <- sapply(z$phrase, KN_tri_all)
result <- z$phrase[which.max(z$prob)]
return(result)
}
getwd()
setwd("~/JHU_DataScience/CapstoneProject/Coursera-SwiftKey/final/en_US")
counts1 <- readRDS("medium_rev_counts1.rds")
counts1 <- select(counts1, word1)
counts1 <- filter(counts1, word1 != "BOS")
lengcount <- dim(counts1)[1]
maxprob <- function(wrd1, wrd2){
x <- paste(wrd1,wrd2, sep = " ")
y <- rep(x, lengcount)
z <- cbind(y, counts1)
z <- mutate(z, phrase = paste(y, word1))
z <- select(z, phrase)
z$prob <- sapply(z$phrase, KN_tri_all)
result <- z$phrase[which.max(z$prob)]
return(result)
}
maxprob("walking", "from")
x <- paste("walking", "from", sep = " ")
y <- rep(x, lengcount)
z <- cbind(y, counts1)
z <- mutate(z, phrase = paste(y, word1))
z <- select(z, phrase)
z$prob <- sapply(z$phrase, KN_tri_all)
source('~/JHU_DataScience/CapstoneProject/Coursera-SwiftKey/final/en_US/medium_rev_kneser_ney_trigram_prediction_model_cmscu.R', echo=TRUE)
?renderText
runApp('~/JHU_DataScience/CapstoneProject/WordPredictor/WordPredictor')
?strsplit
cleanup <- function(line) {
# lower‐case everything
str <- tolower(line);
# strip‐out small html tags
str <- gsub('<[^>]{1,2}>', '', str);
# get rid of all terminal punctuation with a period
str <- gsub('[[:space:]]*[.?!:;]+[[:space:]]*', '', str);
# get rid of anything not A‐Z, ', or whitespace
str <- gsub('[^A‐Z\'[:space:]]', ' ', str);
# collapse whitespace
str <- gsub('[[:space:]]+', ' ', str);
# make sure contraction's are "tight"
str <- gsub(" ?' ?", "'", str);
# make sure terminal . are tight
#str <- gsub(' ?\\. ?', '.', str);
return(str);
}
cleanup(Can this sentence be   cleaned up ?)
cleanup("Can this sentence be   cleaned up ?")
phrase = "Can this sentence be   cleaned up ?"
cleanup(phrase)
cleanup <- function(line) {
# lower‐case everything
str <- tolower(line);
# strip‐out small html tags
#str <- gsub('<[^>]{1,2}>', '', str);
# # get rid of all terminal punctuation with a period
# str <- gsub('[[:space:]]*[.?!:;]+[[:space:]]*', '', str);
# # get rid of anything not A‐Z, ', or whitespace
# str <- gsub('[^A‐Z\'[:space:]]', ' ', str);
# # collapse whitespace
# str <- gsub('[[:space:]]+', ' ', str);
# # make sure contraction's are "tight"
# str <- gsub(" ?' ?", "'", str);
# # make sure terminal . are tight
# #str <- gsub(' ?\\. ?', '.', str);
return(str);
}
cleanup(phrase)
cleanup <- function(line) {
# lower‐case everything
str <- tolower(line);
# strip‐out small html tags
str <- gsub('<[^>]{1,2}>', '', str);
# # get rid of all terminal punctuation with a period
# str <- gsub('[[:space:]]*[.?!:;]+[[:space:]]*', '', str);
# # get rid of anything not A‐Z, ', or whitespace
# str <- gsub('[^A‐Z\'[:space:]]', ' ', str);
# # collapse whitespace
# str <- gsub('[[:space:]]+', ' ', str);
# # make sure contraction's are "tight"
# str <- gsub(" ?' ?", "'", str);
# # make sure terminal . are tight
# #str <- gsub(' ?\\. ?', '.', str);
return(str);
}
phrase = "Can this sentence be >  cleaned up ?"
cleanup(phrase)
cleanup <- function(line) {
# lower‐case everything
str <- tolower(line);
# get rid of all terminal punctuation with a period
# str <- gsub('[[:space:]]*[.?!:;]+[[:space:]]*', '', str);
# get rid of anything not A‐Z, ', or whitespace
str <- gsub('[^A‐Z\'[:space:]]', ' ', str);
# # collapse whitespace
# str <- gsub('[[:space:]]+', ' ', str);
# # make sure contraction's are "tight"
# str <- gsub(" ?' ?", "'", str);
# # make sure terminal . are tight
# #str <- gsub(' ?\\. ?', '.', str);
return(str);
}
cleanup(phrase)
str <- gsub('[^A‐Z\'[:space:]]', '', phrase)
str
str <- gsub('[^A‐Z\\'[:space:]]', '', phrase)
cleanup <- function(line) {
# lower‐case everything
str <- tolower(line);
# get rid of all terminal punctuation with a period
# str <- gsub('[[:space:]]*[.?!:;]+[[:space:]]*', '', str);
# get rid of anything not A‐Z, ', or whitespace
str <- gsub('[^a‐z\'[:space:]]', '', str);
# # collapse whitespace
# str <- gsub('[[:space:]]+', ' ', str);
# # make sure contraction's are "tight"
# str <- gsub(" ?' ?", "'", str);
# # make sure terminal . are tight
# #str <- gsub(' ?\\. ?', '.', str);
return(str);
}
cleanup(phrase)
cleanup <- function(line) {
# lower‐case everything
str <- tolower(line);
# get rid of anything not a‐z, ', or whitespace
str <- gsub('[^a‐z\'[:space:]]', '', str);
# collapse whitespace
str <- gsub('[[:space:]]+', ' ', str);
# # make sure contraction's are "tight"
str <- gsub(" ?' ?", "'", str);
# # make sure terminal . are tight
# #str <- gsub(' ?\\. ?', '.', str);
return(str);
}
cleanup(phrase)
phrase
phrase2 <- cleanup(phrase)
phrase2
gsub("^\\s+|\\s+$", "", phrase2)
phrase
phrase <- "I'd like  this sentence be to be  > # @ cleaned up !"
cleanup <- function(line) {
# lower‐case everything
str <- tolower(line);
# get rid of anything not a‐z, ', or whitespace
str <- gsub('[^a‐z\'[:space:]]', '', str);
# collapse whitespace
str <- gsub('[[:space:]]+', ' ', str);
# # make sure contraction's are "tight"
str <- gsub(" ?' ?", "'", str);
# get rid of leading and trailing whitespace
str <- gsub("^\\s+|\\s+$", "", str)
return(str);
}
cleanup(phrase)
phrase <- "I 'd like  this sentence be to be  > # @ cleaned up !"
cleanup(phrase)
phrase <- "I ''d like  this sentence be to be  > # @ cleaned up !"
cleanup(phrase)
phrase <- "I "d like  this sentence be to be  > # @ cleaned up !"
phrase <- 'I "d like  this sentence be to be  > # @ cleaned up !'
cleanup(phrase)
wc(cleanup(phrase))
tail(strsplit(phrase,split=" ")[[1]],1)
phrase2 <- cleanup(phrase)
phrase2
tail(strsplit(phrase2, split=" ")[[1]],1)
tail(strsplit(phrase2, split=" ")[[1]],2)
lasttwo <- tail(strsplit(phrase2, split=" ")[[1]],2)
lasttwo
maxprob <- function(phrase){
phrase <- cleanup(phrase)
lasttwo <- tail(strsplit(phrase, split=" ")[[1]],2)
y <- rep(lasttwo, lengcount)
z <- cbind(y, counts1)
z <- mutate(z, newphrase = paste(y, word1))
z <- select(z, newphrase)
z$prob <- sapply(z$newphrase, KN_tri_all)
result <- z$newphrase[which.max(z$prob)]
return(result)
}
maxprob <- function(phrse){
phrase <- cleanup(phrse)
lasttwo <- tail(strsplit(phrse, split=" ")[[1]],2)
y <- rep(lasttwo, lengcount)
z <- cbind(y, counts1)
z <- mutate(z, newphrase = paste(y, word1))
z <- select(z, newphrase)
z$prob <- sapply(z$newphrase, KN_tri_all)
result <- z$newphrase[which.max(z$prob)]
return(result
maxprob(phrase)
phrase
phrase <- "I'd like  this sentence be to be  > # @ cleaned up !"
maxprob(phrase)
phrase <- cleanup(phrase)
phrase
lasttwo <- tail(strsplit(phrase, split=" ")[[1]],2)
y <- rep(lasttwo, lengcount)
y[1:20]
?repeatable
?rep
wordvec <- c(lasttwo)
y <- rep(wordvec, lengcount)
y <- rep(wordvec, lengcount, times = 1)
y[1:10]
phrase <- cleanup(phrase)
lasttwo <- tail(strsplit(phrase, split=" ")[[1]],2)
wordvec <- c(lasttwo)
y <- rep(wordvec, lengcount, times = 1)
z <- cbind(y, counts1)
head(z)
lasttwo
phrase <- cleanup(phrase)
lasttwo <- tail(strsplit(phrase, split=" ")[[1]],2)
lasttwo <- paste(lasttwo[1], lasttwo[2], sep = " ")
y <- rep(wordvec, lengcount, times = 1)
wordvec <- c(lasttwo)
y <- rep(wordvec, lengcount, times = 1)
y[1:10]
z <- cbind(y, counts1)
y <- as.data.set(y)
y <- as.data.frame(y)
z <- cbind(y, counts1)
head(z)
z <- mutate(z, newphrase = paste(y, word1))
z <- select(z, newphrase)
z$prob <- sapply(z$newphrase, KN_tri_all)
result <- z$newphrase[which.max(z$prob)]
result
counts1 <- readRDS("medium_rev_counts1.rds")
counts1 <- select(counts1, word1)
counts1 <- filter(counts1, word1 != "BOS" & word1 != "EOS")
lengcount <- dim(counts1)[1]
maxprob <- function(x){
x <- cleanup(x)
lasttwo <- tail(strsplit(x, split=" ")[[1]],2)
lasttwo <- paste(lasttwo[1], lasttwo[2], sep = " ")
wordvec <- c(lasttwo)
y <- rep(wordvec, lengcount, times = 1)
y <- as.data.frame(y)
z <- cbind(y, counts1)
z <- mutate(z, newphrase = paste(y, word1))
z <- select(z, newphrase)
z$prob <- sapply(z$newphrase, KN_tri_all)
result <- z$newphrase[which.max(z$prob)]
}
maxprob(phrase)
maxprob <- function(x){
x <- cleanup(x)
lasttwo <- tail(strsplit(x, split=" ")[[1]],2)
lasttwo <- paste(lasttwo[1], lasttwo[2], sep = " ")
wordvec <- c(lasttwo)
y <- rep(wordvec, lengcount, times = 1)
y <- as.data.frame(y)
z <- cbind(y, counts1)
z <- mutate(z, newphrase = paste(y, word1))
z <- select(z, newphrase)
z$prob <- sapply(z$newphrase, KN_tri_all)
result <- z$newphrase[which.max(z$prob)]
return(result)
}
maxprob(phrase)
?Rprof
Rprof(maxprob(phrase))
x <- cleanup(phrase)
lasttwo <- tail(strsplit(x, split=" ")[[1]],2)
lasttwo <- paste(lasttwo[1], lasttwo[2], sep = " ")
y <- as.data.frame(rep(wordvec, lengcount, times = 1))
y[1:10,]
z <- cbind(y, counts1)
z$newphrase <- paste(z$y, z$word1, sep = " ")
z$prob <- sapply(z$newphrase, KN_tri_all)
head(z)
x <- cleanup(phrase)
lasttwo <- tail(strsplit(x, split=" ")[[1]],2)
lasttwo <- paste(lasttwo[1], lasttwo[2], sep = " ")
y <- rep(wordvec, lengcount, times = 1)
y <- as.data.frame(y)
z <- cbind(y, counts1)
z$newphrase <- paste(z$y, z$word1, sep = " ")
z$prob <- sapply(z$newphrase, KN_tri_all)
z$prob <- sapply(z$newphrase, KN_tri_all)
result <- z$newphrase[which.max(z$prob)]
x <- cleanup(phrase)
lasttwo <- tail(strsplit(x, split=" ")[[1]],2)
lasttwo <- paste(lasttwo[1], lasttwo[2], sep = " ")
#wordvec <- c(lasttwo)
y <- rep(wordvec, lengcount, times = 1)
y <- as.data.frame(y)
z <- cbind(y, counts1)
z$newphrase <- paste(z$y, z$word1, sep = " ")
z <- data.table(z)
z$prob <- sapply(z$newphrase, KN_tri_all)
gc()
z$prob <- sapply(z$newphrase, KN_tri_all)
z[, prob] := sapply(z[, newphrase], KN_tri_all)
z <- data.table(z)
head(z)
is.data.table(z)
z[, prob] := lapply(z[, newphrase], KN_tri_all)
help(":=")
z[, prob] <- sapply(z[, newphrase], KN_tri_all)
z[, prob] <- sapply(z[, newphrase], KN_tri_all)
z[, prob := lapply(newphrase, KN_tri_all)]
z[, prob := lapply(newphrase, KN_tri_all)]
setwd("~/JHU_DataScience/CapstoneProject/WordPredictor/WordPredictor")
?saveRDS
source('~/JHU_DataScience/CapstoneProject/Coursera-SwiftKey/final/en_US/abbrviated counts for shiny app.R', echo=TRUE)
getwd()
?withProgress
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
?textOutput
runApp()
?tag
runApp()
?container
runApp()
maxprob <- function(x){
x <- cleanup(x)
lasttwo <- tail(strsplit(x, split=" ")[[1]],2)
lasttwo <- paste(lasttwo[1], lasttwo[2], sep = " ")
#wordvec <- c(lasttwo)
y <- as.data.frame(rep(wordvec, lengcount, times = 1))
#y <- as.data.frame(y)
z <- cbind(y, counts1)
z$newphrase <- paste(z$y, z$word1, sep = " ")
#z <- mutate(z, newphrase = paste(y, word1))
#z <- select(z, newphrase)
z$prob <- sapply(z$newphrase, KN_tri_all)
result <- z$newphrase[which.max(z$prob)]
return(result)
}
maxprob(Yesterday I read a)
maxprob("Yesterday I read a")"
maxprob("Yesterday I read a")
x <- "Yesterday I read a"
x <- cleanup(x)
lasttwo <- tail(strsplit(x, split=" ")[[1]],2)
lasttwo <- paste(lasttwo[1], lasttwo[2], sep = " ")
y <- as.data.frame(rep(wordvec, lengcount, times = 1))
z <- cbind(y, counts1)
z$newphrase <- paste(z$y, z$word1, sep = " ")
head(z)
x <- "Yesterday I read a"
x <- cleanup(x)
lasttwo <- tail(strsplit(x, split=" ")[[1]],2)
lasttwo <- paste(lasttwo[1], lasttwo[2], sep = " ")
y <- rep(wordvec, lengcount, times = 1)
y <- as.data.frame(y)
head(y)
z <- cbind(y, counts1)
z$newphrase <- paste(z$y, z$word1, sep = " ")
head(z)
z$prob <- sapply(z$newphrase, KN_tri_all)
z$prob <- sapply(z$newphrase, KN_tri_all)
runApp()
runApp()
phrase
rm(please)
rm(phrase)
runApp()
runApp()
runApp()
runApp()
